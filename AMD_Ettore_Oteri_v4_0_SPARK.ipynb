{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmlgZlDiETM5xITzknJE+f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ettoreotery/AMD-project-DSE-25/blob/main/AMD_Ettore_Oteri_v4_0_SPARK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KpkXBXXNWCc",
        "outputId": "e8219855-7551-44bc-d73c-db77387d68f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to /content\n",
            " 98% 1.05G/1.06G [00:07<00:00, 345MB/s]\n",
            "100% 1.06G/1.06G [00:07<00:00, 149MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"ettoreoteri\"\n",
        "os.environ['KAGGLE_KEY'] = \"xxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews\n",
        "!unzip -q \"*.zip\" -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_csv(\"Books_rating.csv\")\n",
        "df1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "DoqZQoqGNhdv",
        "outputId": "0a9f0811-37cc-4784-c139-c3852a94f822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Id                           Title  Price         User_id  \\\n",
              "0  1882931173  Its Only Art If Its Well Hung!    NaN   AVCGYZL8FQQTD   \n",
              "1  0826414346        Dr. Seuss: American Icon    NaN  A30TK6U7DNS82R   \n",
              "2  0826414346        Dr. Seuss: American Icon    NaN  A3UH4UZ4RSVO82   \n",
              "3  0826414346        Dr. Seuss: American Icon    NaN  A2MVUWT453QH61   \n",
              "4  0826414346        Dr. Seuss: American Icon    NaN  A22X4XUPKF66MR   \n",
              "\n",
              "                          profileName review/helpfulness  review/score  \\\n",
              "0               Jim of Oz \"jim-of-oz\"                7/7           4.0   \n",
              "1                       Kevin Killian              10/10           5.0   \n",
              "2                        John Granger              10/11           5.0   \n",
              "3  Roy E. Perry \"amateur philosopher\"                7/7           4.0   \n",
              "4     D. H. Richards \"ninthwavestore\"                3/3           4.0   \n",
              "\n",
              "   review/time                                   review/summary  \\\n",
              "0    940636800           Nice collection of Julie Strain images   \n",
              "1   1095724800                                Really Enjoyed It   \n",
              "2   1078790400  Essential for every personal and Public Library   \n",
              "3   1090713600  Phlip Nel gives silly Seuss a serious treatment   \n",
              "4   1107993600                           Good academic overview   \n",
              "\n",
              "                                         review/text  \n",
              "0  This is only for Julie Strain fans. It's a col...  \n",
              "1  I don't care much for Dr. Seuss but after read...  \n",
              "2  If people become the books they read and if \"t...  \n",
              "3  Theodore Seuss Geisel (1904-1991), aka &quot;D...  \n",
              "4  Philip Nel - Dr. Seuss: American IconThis is b...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-274f1c64-3f59-410b-949d-356366c70a88\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Title</th>\n",
              "      <th>Price</th>\n",
              "      <th>User_id</th>\n",
              "      <th>profileName</th>\n",
              "      <th>review/helpfulness</th>\n",
              "      <th>review/score</th>\n",
              "      <th>review/time</th>\n",
              "      <th>review/summary</th>\n",
              "      <th>review/text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1882931173</td>\n",
              "      <td>Its Only Art If Its Well Hung!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AVCGYZL8FQQTD</td>\n",
              "      <td>Jim of Oz \"jim-of-oz\"</td>\n",
              "      <td>7/7</td>\n",
              "      <td>4.0</td>\n",
              "      <td>940636800</td>\n",
              "      <td>Nice collection of Julie Strain images</td>\n",
              "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0826414346</td>\n",
              "      <td>Dr. Seuss: American Icon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A30TK6U7DNS82R</td>\n",
              "      <td>Kevin Killian</td>\n",
              "      <td>10/10</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1095724800</td>\n",
              "      <td>Really Enjoyed It</td>\n",
              "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0826414346</td>\n",
              "      <td>Dr. Seuss: American Icon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A3UH4UZ4RSVO82</td>\n",
              "      <td>John Granger</td>\n",
              "      <td>10/11</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1078790400</td>\n",
              "      <td>Essential for every personal and Public Library</td>\n",
              "      <td>If people become the books they read and if \"t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0826414346</td>\n",
              "      <td>Dr. Seuss: American Icon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A2MVUWT453QH61</td>\n",
              "      <td>Roy E. Perry \"amateur philosopher\"</td>\n",
              "      <td>7/7</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1090713600</td>\n",
              "      <td>Phlip Nel gives silly Seuss a serious treatment</td>\n",
              "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0826414346</td>\n",
              "      <td>Dr. Seuss: American Icon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A22X4XUPKF66MR</td>\n",
              "      <td>D. H. Richards \"ninthwavestore\"</td>\n",
              "      <td>3/3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1107993600</td>\n",
              "      <td>Good academic overview</td>\n",
              "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-274f1c64-3f59-410b-949d-356366c70a88')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-274f1c64-3f59-410b-949d-356366c70a88 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-274f1c64-3f59-410b-949d-356366c70a88');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5069f534-ad09-4623-b666-c25cd8c57790\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5069f534-ad09-4623-b666-c25cd8c57790')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5069f534-ad09-4623-b666-c25cd8c57790 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df1"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"This dataset contains {df1.shape[0]} rows and {df1.shape[1]} columns.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b-s_-9HNqrY",
        "outputId": "bd6551e1-ab12-4e47-c37d-9ecf0a2e3eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset contains 3000000 rows and 10 columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark nltk\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install datasketch\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from itertools import combinations\n",
        "import random\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS8ek3Z6NvBx",
        "outputId": "1558f094-9713-4720-8fe0-81f1fa7c8822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting datasketch\n",
            "  Downloading datasketch-1.6.5-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.11/dist-packages (from datasketch) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasketch) (1.15.3)\n",
            "Downloading datasketch-1.6.5-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: datasketch\n",
            "Successfully installed datasketch-1.6.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BookReviewsJaccard\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "SAux4ubZN3E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsampling\n",
        "SAMPLE_SIZE = 10000  # Sample size when subsampling\n",
        "SEED = 42\n",
        "USE_FULL_DATA = False  # Set to True to disable subsampling"
      ],
      "metadata": {
        "id": "qPWg8jU5N71B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing with error handling\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    try:\n",
        "        if not isinstance(text, str):\n",
        "            return []\n",
        "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "        tokens = word_tokenize(text)\n",
        "        return [lemmatizer.lemmatize(w) for w in tokens\n",
        "               if w not in stop_words and len(w) > 2]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# Load function to extract only review texts\n",
        "def load_review_texts():\n",
        "    # Read the review/text column\n",
        "    lines = sc.textFile(\"Books_rating.csv\")\n",
        "    header = lines.first()\n",
        "\n",
        "    # Extract only the review text column\n",
        "    review_lines = lines.filter(lambda line: line != header) \\\n",
        "                        .map(lambda line: line.split('\"')[-2] if '\"' in line else line.split(',')[-1])\n",
        "\n",
        "    review_lines = review_lines.filter(lambda x: len(x) > 10).distinct()\n",
        "\n",
        "    # Sampling logic\n",
        "    sampled_reviews = review_lines.collect() if USE_FULL_DATA \\\n",
        "                      else review_lines.takeSample(False, SAMPLE_SIZE, SEED)\n",
        "\n",
        "    print(f\"Number of unique reviews considered for sampling: {len(sampled_reviews)}\")\n",
        "\n",
        "    # Process sampled reviews\n",
        "    processed_reviews = []\n",
        "    for text in sampled_reviews:\n",
        "        processed = preprocess_text(text)\n",
        "        if len(processed) >= 5:  # Only keep reviews with enough tokens\n",
        "            processed_reviews.append((text, processed))\n",
        "\n",
        "    return sc.parallelize(processed_reviews, numSlices=8)\n",
        "\n",
        "# Process data\n",
        "reviews_rdd = load_review_texts().cache()\n",
        "\n",
        "print(f\"Sample size loaded: {reviews_rdd.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNPjgz3FOBoD",
        "outputId": "d45c943f-6a01-4b4f-e495-bdac213c35cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique reviews considered for sampling: 10000\n",
            "Sample size loaded: 9773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity calculation\n",
        "def get_minhash(tokens, num_perm=128):\n",
        "    m = MinHash(num_perm=num_perm)\n",
        "    for token in tokens:\n",
        "        m.update(token.encode('utf8'))\n",
        "    return m\n",
        "\n",
        "# Generate and process pairs\n",
        "def process_pairs(rdd, num_perm=128, threshold=0.5):\n",
        "    data = rdd.collect()\n",
        "\n",
        "    minhashes = []\n",
        "    for i, (original_text, tokens) in enumerate(data):\n",
        "        mh = get_minhash(tokens, num_perm)\n",
        "        minhashes.append((i, original_text, tokens, mh))\n",
        "\n",
        "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "    for i, _, _, mh in minhashes:\n",
        "        lsh.insert(str(i), mh)\n",
        "\n",
        "    results = []\n",
        "    for i, text1, tokens1, mh1 in minhashes:\n",
        "        for j in lsh.query(mh1):\n",
        "            if int(j) > i:\n",
        "                text2, tokens2, mh2 = minhashes[int(j)][1:4]\n",
        "                set1, set2 = set(tokens1), set(tokens2)\n",
        "                intersection = set1 & set2\n",
        "                union = set1 | set2\n",
        "                jaccard = len(intersection) / len(union) if union else 0.0\n",
        "                if jaccard > 0:\n",
        "                    results.append((text1, text2, jaccard, intersection))\n",
        "\n",
        "    results.sort(key=lambda x: -x[2])\n",
        "    return results[:20]\n",
        "\n",
        "# Get top pairs\n",
        "top_pairs = process_pairs(reviews_rdd)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nTOP 20 MOST SIMILAR PAIRS:\")\n",
        "for idx, (text1, text2, sim, common_tokens) in enumerate(top_pairs, 1):\n",
        "    print(f\"\\n#{idx}: Similarity = {sim:.4f}\")\n",
        "    print(\"\\nReview 1:\")\n",
        "    print(text1)\n",
        "    print(\"\\nReview 2:\")\n",
        "    print(text2)\n",
        "    print(f\"\\nCommon tokens ({len(common_tokens)}): {common_tokens}\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFQxUY-1OE8f",
        "outputId": "0fd65109-9988-4d51-f4e4-8072325254d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TOP 20 MOST SIMILAR PAIRS:\n",
            "\n",
            "#1: Similarity = 0.8163\n",
            "\n",
            "Review 1:\n",
            "Fun books for their illustrations and not their writing quality, the Billy and Blaze series were always at the top of my list growing up. This book was one of my favorites, dripping with adventure and exploration.Although poorly written, the Billy and Blaze series was fascinating to me as a girl and fed my imagination. Any young horse lovers will eat up the vivid black and white drawings that tell the quaint stories much more eloquently than the rather dry words.To read more of my children's book reviews, visit larsenreviews.org.\n",
            "\n",
            "Review 2:\n",
            "Fun books for their illustrations and not their writing quality, the Billy and Blaze series were always at the top of my list growing up. They smack of great wild west adventures and the fun that can be had on horseback.Although poorly written, the Billy and Blaze series was fascinating to me as a girl and fed my imagination. Any young horse lovers will eat up the vivid black and white drawings that tell the quaint stories much more eloquently than the rather dry words.To read more of my children's book reviews, visit larsenreviews.org.\n",
            "\n",
            "Common tokens (40): {'childrens', 'lover', 'dry', 'much', 'larsenreviewsorg', 'book', 'always', 'rather', 'poorly', 'illustration', 'young', 'growing', 'girl', 'vivid', 'imagination', 'quality', 'white', 'fun', 'story', 'tell', 'series', 'visit', 'writing', 'read', 'fed', 'horse', 'review', 'blaze', 'written', 'black', 'adventure', 'top', 'drawing', 'quaint', 'billy', 'wordsto', 'fascinating', 'eloquently', 'eat', 'list'}\n",
            "====================================================================================================\n",
            "\n",
            "#2: Similarity = 0.5556\n",
            "\n",
            "Review 1:\n",
            "Wonderful sequel to Ender's Shadow - can't wait for the next 2 books in the series.\n",
            "\n",
            "Review 2:\n",
            ",can't wait for the next book in this series.\n",
            "\n",
            "Common tokens (5): {'next', 'cant', 'series', 'wait', 'book'}\n",
            "====================================================================================================\n",
            "\n",
            "#3: Similarity = 0.4444\n",
            "\n",
            "Review 1:\n",
            "I was very impress with this book it was also the best book i've read about wv.\n",
            "\n",
            "Review 2:\n",
            ". If you don't have this book you should get it because it's one of the best books I've read!\n",
            "\n",
            "Common tokens (4): {'ive', 'best', 'book', 'read'}\n",
            "====================================================================================================\n",
            "\n",
            "#4: Similarity = 0.3333\n",
            "\n",
            "Review 1:\n",
            " and I look forward to reading more great books by this talented author.\n",
            "\n",
            "Review 2:\n",
            " and look forward to reading it.Coleen from Kent, Wa\n",
            "\n",
            "Common tokens (3): {'reading', 'forward', 'look'}\n",
            "====================================================================================================\n",
            "\n",
            "#5: Similarity = 0.3158\n",
            "\n",
            "Review 1:\n",
            "I have not had time to read this book yet. But I do know that Janette Oke is a fantastic author. I have read many of her books. I will enjoy reading this book.\n",
            "\n",
            "Review 2:\n",
            "I've seen the movies read the series who knows how many times but reading the first book is always a adventure\n",
            "\n",
            "Common tokens (6): {'reading', 'know', 'many', 'time', 'read', 'book'}\n",
            "====================================================================================================\n",
            "\n",
            "#6: Similarity = 0.3158\n",
            "\n",
            "Review 1:\n",
            "I think this was a VERY good book. The plot was very well thought out, the characters were interesting and the whole story was intriguing.\n",
            "\n",
            "Review 2:\n",
            "I thought it was excellent material for good actors. There are unstereotyped characters and a great story, about the loss of innocence and guilt. A most interesting book\n",
            "\n",
            "Common tokens (6): {'character', 'good', 'story', 'interesting', 'thought', 'book'}\n",
            "====================================================================================================\n",
            "\n",
            "#7: Similarity = 0.3125\n",
            "\n",
            "Review 1:\n",
            "I read this book back when I was in high school and just recently reread it just as awesome now as it was then!\n",
            "\n",
            "Review 2:\n",
            "I hate reading. As a high school senior, there is no book that i would ever pick up and read on my own. After reading Airframe, i would re-read it again any day...UNBELIEVABLE BOOK!\n",
            "\n",
            "Common tokens (5): {'reread', 'school', 'high', 'read', 'book'}\n",
            "====================================================================================================\n",
            "\n",
            "#8: Similarity = 0.3077\n",
            "\n",
            "Review 1:\n",
            "The product arrived in as good as expected condition, it was exactly has the seller described it.\n",
            "\n",
            "Review 2:\n",
            "This product arrived quickly and was exactly how it was described. Thank you so much for the honesty and for having it available.\n",
            "\n",
            "Common tokens (4): {'exactly', 'product', 'described', 'arrived'}\n",
            "====================================================================================================\n",
            "\n",
            "#9: Similarity = 0.2727\n",
            "\n",
            "Review 1:\n",
            ". If you don't have this book you should get it because it's one of the best books I've read!\n",
            "\n",
            "Review 2:\n",
            "I already had this book and have read it over and over several times.Purchased this one as a gift.\n",
            "\n",
            "Common tokens (3): {'book', 'one', 'read'}\n",
            "====================================================================================================\n",
            "\n",
            "#10: Similarity = 0.2727\n",
            "\n",
            "Review 1:\n",
            "The person I bought the book for enjoyed the book and praised the information on grief.\n",
            "\n",
            "Review 2:\n",
            "I bought this book for my grandchildren. I thought it was very creative and my grandchildren enjoyed it. They are age 4 and 2.\n",
            "\n",
            "Common tokens (3): {'enjoyed', 'bought', 'book'}\n",
            "====================================================================================================\n",
            "\n",
            "#11: Similarity = 0.2727\n",
            "\n",
            "Review 1:\n",
            "I read this book.... and I couldn't even finish, it was so bad. Don't buy this one.\n",
            "\n",
            "Review 2:\n",
            " when you finish this one. I got so much out of this book.\n",
            "\n",
            "Common tokens (3): {'book', 'one', 'finish'}\n",
            "====================================================================================================\n",
            "\n",
            "#12: Similarity = 0.2667\n",
            "\n",
            "Review 1:\n",
            ". If you don't have this book you should get it because it's one of the best books I've read!\n",
            "\n",
            "Review 2:\n",
            "This book was very interesting, I'll never regret reading this! This book had a lot of imagination in it, it is one of the best books I ever read.\n",
            "\n",
            "Common tokens (4): {'one', 'best', 'book', 'read'}\n",
            "====================================================================================================\n",
            "\n",
            "#13: Similarity = 0.2500\n",
            "\n",
            "Review 1:\n",
            "This book was a good read. At times I was bored with the writer, but over all it was a great book!\n",
            "\n",
            "Review 2:\n",
            "top 5 books ever. i like to read it around christmas time.\n",
            "\n",
            "Common tokens (3): {'time', 'book', 'read'}\n",
            "====================================================================================================\n",
            "\n",
            "#14: Similarity = 0.2500\n",
            "\n",
            "Review 1:\n",
            "I read it in school and I hate reading but i couldn't put it down.\n",
            "\n",
            "Review 2:\n",
            "The story of Redwall is so facinating I read it in 2 days. I was up to 1:00 am reading, I couldn't put it down. This is the number 1 book that I would recomend to anybody!\n",
            "\n",
            "Common tokens (4): {'reading', 'couldnt', 'put', 'read'}\n",
            "====================================================================================================\n",
            "\n",
            "#15: Similarity = 0.2273\n",
            "\n",
            "Review 1:\n",
            "This book tells the story of two hobos that are trying to make a living. I would recomend reading this book because it is a good book. I liked this book.\n",
            "\n",
            "Review 2:\n",
            "I never liked reading books but since reading this one on my kindle I'm about to change my sentiment. Good balanced story - could not put the 'book' down.\n",
            "\n",
            "Common tokens (5): {'reading', 'story', 'good', 'liked', 'book'}\n",
            "====================================================================================================\n",
            "\n",
            "#16: Similarity = 0.2222\n",
            "\n",
            "Review 1:\n",
            "It's a great book, plenty if recipes really easy and with a lot of variety. It has influences from a lot of different countries and the plates are very healthy. A lot of good ideas.\n",
            "\n",
            "Review 2:\n",
            "I love this book! The recipes are easy to make, taste great, and they are good for you! I make them for a healthy, filling breakfast or a workout snack - it's cheaper to make them yourself than to buy them at the gym. I strongly recommend this book.\n",
            "\n",
            "Common tokens (6): {'good', 'recipe', 'healthy', 'easy', 'great', 'book'}\n",
            "====================================================================================================\n",
            "\n",
            "#17: Similarity = 0.2222\n",
            "\n",
            "Review 1:\n",
            "this book is a pure gold. Peggy knows deeply what eating disorders are all about. and from a very personal experiance- it works . it saved my life.\n",
            "\n",
            "Review 2:\n",
            "I would recommend this book to anyone who knows people with eating disorders or anyone who is interested in them.\n",
            "\n",
            "Common tokens (4): {'book', 'disorder', 'know', 'eating'}\n",
            "====================================================================================================\n",
            "\n",
            "#18: Similarity = 0.2174\n",
            "\n",
            "Review 1:\n",
            "I have not had time to read this book yet. But I do know that Janette Oke is a fantastic author. I have read many of her books. I will enjoy reading this book.\n",
            "\n",
            "Review 2:\n",
            "I hated for this book to end! Ms. Penman took me - heart, mind, and soul - back in time. I have read many of her books and will continue reading them as long as she writes them!\n",
            "\n",
            "Common tokens (5): {'reading', 'many', 'time', 'read', 'book'}\n",
            "====================================================================================================\n",
            "\n",
            "#19: Similarity = 0.2105\n",
            "\n",
            "Review 1:\n",
            "Really enjoyed this book, from beginning to end. Think it's one of her best.\n",
            "\n",
            "Review 2:\n",
            " the same at the beginning of the book as she does at the end, meaning their is no growth in her speach pattern. There is some growth in the character, but I just didn't find her story all that compelling. It's not a terrible book, but I didn't think it was a great book.\n",
            "\n",
            "Common tokens (4): {'beginning', 'think', 'book', 'end'}\n",
            "====================================================================================================\n",
            "\n",
            "#20: Similarity = 0.2083\n",
            "\n",
            "Review 1:\n",
            "I hated for this book to end! Ms. Penman took me - heart, mind, and soul - back in time. I have read many of her books and will continue reading them as long as she writes them!\n",
            "\n",
            "Review 2:\n",
            "I've seen the movies read the series who knows how many times but reading the first book is always a adventure\n",
            "\n",
            "Common tokens (5): {'reading', 'many', 'time', 'read', 'book'}\n",
            "====================================================================================================\n"
          ]
        }
      ]
    }
  ]
}